{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Spacy en Español",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMAN/BXZihe28dRZ0nlCUGl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Metaboll/Python-Spacy-Spanish-Language/blob/master/Spacy_en_Espa%C3%B1ol.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kwm6QJFHB75j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#\n",
        "#Trabajo de Spacy en Español\n",
        "#\n",
        "# Determinar sujeto, complementos directo y circunstanciales\n",
        "\n",
        "# Principios de Abril 2020\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "urCrL5NDGK-4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install -U spacy\n",
        "print(\"***\"*10)\n",
        "!python -m spacy download es_core_news_sm\n",
        "#es_core_news_md"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tEYBQcj8GREH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!python -m spacy download es_core_news_md"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I6N05iVUJH3v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk import Tree\n",
        "#https://stackoverflow.com/questions/39323325/can-i-find-subject-from-spacy-dependency-tree-using-nltk-in-python\n",
        "\n",
        "\n",
        "def to_nltk_tree(node):\n",
        "    if node.n_lefts + node.n_rights > 0:\n",
        "        return Tree(node.orth_, [to_nltk_tree(child) for child in node.children])\n",
        "    else:\n",
        "        return node.orth_\n",
        "\n",
        " [to_nltk_tree(sent.root).pretty_print() for sent in doc.sents]       "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7r6xTLa7M7cP",
        "colab_type": "code",
        "outputId": "e338eada-5b26-4108-99a2-00c5a1afc376",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "############################# LOCALIZA COMPLEMENTOS DIRECTOS, INDIRECTOS,, ETC\n",
        "import es_core_news_sm\n",
        "nlp = es_core_news_sm.load()\n",
        "\n",
        "\n",
        "doc = nlp(\"Maria vende estos lapiceros en la tienda\")\n",
        "doc = nlp(\"Yo me llamo Fernando Garcia\") # Yo nsubj /Fernando Garcia Es obj\n",
        "#doc = nlp(\"Mi coche es rojo\")\n",
        "#doc = nlp(\"Yo es encima de la mesa\") # Yo nsubj / la mesa es nmod\n",
        "#doc = nlp(\"El llego por la tarde\")# El llego / la tarde nmod\n",
        "#doc = nlp(\"Yo soy Eduardo\")# El llego / la tarde nmod\n",
        "def extrae_complementos(parsed_text):\n",
        "    for chunk in doc.noun_chunks:\n",
        "        #print(chunk.text, chunk.root.text, chunk.root.dep_,chunk.root.head.text)\n",
        "        print(chunk.text,\"  >>>>>>>\", chunk.root.dep_)\n",
        "        \n",
        "print( extrae_complementos(doc))\n",
        "\n",
        "\n",
        "while True:\n",
        "    message=input('You: ')\n",
        "    if message.strip() != 'Bye':\n",
        "        doc = nlp(message)\n",
        "        print(\"*\"*20)\n",
        "        extrae_complementos(doc)\n",
        "    if message.strip() == 'Bye':\n",
        "        print('ChatBot : Bye')\n",
        "        break"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Yo   >>>>>>> nsubj\n",
            "me   >>>>>>> obj\n",
            "Fernando Garcia   >>>>>>> obj\n",
            "None\n",
            "You: Cuando quieres que te recoja\n",
            "********************\n",
            "te   >>>>>>> obj\n",
            "You: Cuando te llamo\n",
            "********************\n",
            "te   >>>>>>> obj\n",
            "You: Cuando me llamas\n",
            "********************\n",
            "me   >>>>>>> obj\n",
            "You: Donde esta el aeropuesto\n",
            "********************\n",
            "Donde   >>>>>>> case\n",
            "el aeropuesto   >>>>>>> ROOT\n",
            "You: Donde esta el aeropuerto\n",
            "********************\n",
            "Donde   >>>>>>> case\n",
            "el aeropuerto   >>>>>>> ROOT\n",
            "You: Eduardo esta en el aeropuerto\n",
            "********************\n",
            "esta   >>>>>>> obj\n",
            "el aeropuerto   >>>>>>> obl\n",
            "You: El libro esta encima de la mesa\n",
            "********************\n",
            "El libro   >>>>>>> ROOT\n",
            "la mesa   >>>>>>> nmod\n",
            "You: El libro es blanco\n",
            "********************\n",
            "El libro   >>>>>>> nsubj\n",
            "You: El libro no es blanco\n",
            "********************\n",
            "El libro   >>>>>>> nsubj\n",
            "You: El tranvia esta en la estacion de tren\n",
            "********************\n",
            "El tranvia   >>>>>>> ROOT\n",
            "la estacion de tren   >>>>>>> nmod\n",
            "You: No hay espacio en el bar\n",
            "********************\n",
            "espacio   >>>>>>> obj\n",
            "el bar   >>>>>>> obl\n",
            "You: No te dejo espacio\n",
            "********************\n",
            "te   >>>>>>> obj\n",
            "espacio   >>>>>>> obj\n",
            "You: Yo no te dejo espacio en el bar\n",
            "********************\n",
            "Yo   >>>>>>> nsubj\n",
            "te   >>>>>>> obj\n",
            "espacio   >>>>>>> obj\n",
            "el bar   >>>>>>> obl\n",
            "You: Cuando quieras te llamo\n",
            "********************\n",
            "te   >>>>>>> obj\n",
            "You: bye\n",
            "********************\n",
            "You: Bye\n",
            "ChatBot : Bye\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OtlQWUgjgxQ7",
        "colab_type": "code",
        "outputId": "9306c2ee-d012-4608-89e8-0067dcb517fe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "#import spacy\n",
        "from spacy.symbols import nsubj, VERB\n",
        "import es_core_news_md\n",
        "nlp = es_core_news_md.load()\n",
        "# mas informacion\n",
        "#https://stackoverflow.com/questions/56275467/tokenisation-with-spacy-how-to-get-left-and-right-tokens\n",
        "\n",
        "\n",
        "\n",
        "#import es_core_news_sm\n",
        "#nlp = es_core_news_sm.load()\n",
        "#doc = nlp(u\"This is some sentence that spacy will not appreciate\")\n",
        "#sub_toks = [tok for tok in doc if (tok.dep_ == \"nsubj\") ]\n",
        "        #This nsubj is VERB []\n",
        "        #is ROOT is VERB [This, sentence]\n",
        "        #some det sentence NOUN []\n",
        "        #sentence attr is VERB [some, appreciate]\n",
        "        #that mark appreciate VERB []\n",
        "        #spacy nsubj appreciate VERB []\n",
        "        #will aux appreciate VERB []\n",
        "        #not neg appreciate VERB []\n",
        "        #appreciate relcl sentence NOUN [that, spacy, will, not]\n",
        "\n",
        "def extrae_sujeto3(parsed_text): # esta funcion no funciona bien del todo. Un poco peor\n",
        "    sub_toks = [tok for tok in doc if (tok.dep_ == \"nsubj\") ]\n",
        "\n",
        "def extrae_sujeto2(parsed_text):\n",
        "    # Finding a verb with a subject from below — good\n",
        "    verbs = set()\n",
        "    for possible_subject in doc:\n",
        "        if possible_subject.dep == nsubj and possible_subject.head.pos == VERB:\n",
        "           #verbs.add(possible_subject.head)\n",
        "           return(possible_subject.text)\n",
        "    return[]\n",
        "\n",
        "def extrae_sujeto(parsed_text):\n",
        "    sujeto = \"\"\n",
        "    for token in doc:\n",
        "        #print(token.text, token.dep_, token.head.text, token.head.pos_, [child for child in token.children])\n",
        "        \n",
        "        a = token.text\n",
        "        b = token.dep_\n",
        "        c = token.head.text\n",
        "        d = token.head.pos_\n",
        "        \n",
        "        #print(\" token.text\", a)\n",
        "        #print(\" token.dep_\", b)\n",
        "        if b == \"ROOT\":\n",
        "            my_list =  [child for child in token.children]\n",
        "            #print(my_list)\n",
        "            for child in token.children:\n",
        "                if child:\n",
        "                    if sujeto == child.text:\n",
        "                        #break\n",
        "                        #print(\"Eureka   \" , sujeto)\n",
        "                        return sujeto\n",
        "        elif b == \"nsubj\":\n",
        "            print(\"Candidato a sujeto\" , token.text)\n",
        "            sujeto = token.text # a\n",
        "        #elif b == \"iobj\":\n",
        "            #indirect_object = token.orth_\n",
        "            #print(\"Obj indi : \", a)\n",
        "         #dobj for direct object\n",
        "        #if b == \"dobj\":\n",
        "            #print(\"Obj dire : \", a)\n",
        "            #direct_object = token.orth_\n",
        "    return []\n",
        "\n",
        "#doc = nlp(u'Yo estoy trabajando hasta el 15 de Abril')\n",
        "doc = nlp(u'Yo me llamo Fernando')\n",
        "#doc = nlp(u'Me llamo Fernando') #  es mejor la funcion mia precisamente por esta frase\n",
        "doc = nlp(u'Fernando corre alto')\n",
        "doc = nlp(u'  Yo naci en España')\n",
        "#doc = nlp(u' yo dispare a un elefante')\n",
        "bYE\n",
        "Bye\n",
        "\n",
        "\n",
        "#extrae_sujeto2(doc)\n",
        "print( \"Solucion 2 : \" , extrae_sujeto2(doc))\n",
        "#string = \"geeks for geeks geeks geeks geeks\" \n",
        "#print(string.replace(\"geeks\", \"Geeks\"))  "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Candidato a sujeto Yo\n",
            "Solucion 1  Yo\n",
            "Solucion 2 :  Yo\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZjwuOraPnSUu",
        "colab_type": "code",
        "outputId": "03a1bf2e-7f18-49f3-e38c-c532bfe16254",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "doc = nlp(u'El objetivo de todo el trabajo es que el programa funcione correctamente')\n",
        "#nmod nsubj\n",
        "print(\"Sujeto \", extrae_sujeto(doc))\n",
        "print( extrae_complementos(doc))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Candidato a sujeto objetivo\n",
            "[objetivo, funcione]\n",
            "Sujeto  objetivo\n",
            "El objetivo objetivo nsubj es\n",
            "el trabajo trabajo nmod objetivo\n",
            "el programa programa nsubj funcione\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "15gi_wO2DChQ",
        "colab_type": "code",
        "outputId": "cb1aed09-3db7-4657-91eb-1740fd6647f2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import spacy\n",
        "from nltk import Tree\n",
        "#https://stackoverflow.com/questions/39323325/can-i-find-subject-from-spacy-dependency-tree-using-nltk-in-python\n",
        "\n",
        "#import es_core_news_sm\n",
        "#nlp = es_core_news_sm.load()\n",
        "print(\" Version de Spacy es \", spacy.__version__)\n",
        "print(\"1mera prueva\")\n",
        "#nlp = spacy.load(\"es_core_news_sm\")\n",
        "#for ent in doc.ents:\n",
        "#    print(ent.text, ent.start_char, ent.end_char, ent.label_)\n",
        "#doc = nlp(u'Apple is looking at buying U.K. startup for $1 billion')\n",
        "doc = nlp(u'Facebook esta trabajando hasta el 15 de Abril')\n",
        "while True:\n",
        "    message=input('You: ')\n",
        "    if message.strip() != 'Bye':\n",
        "        #reply=bot.get_response(message)\n",
        "        doc = nlp(message)\n",
        "        print(\"*\"*20)\n",
        "        #for token in doc:\n",
        "            #print(token.text, token.lemma_, token.pos_,  token.dep_ )\n",
        "        #    print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_,\n",
        "        #        token.shape_, token.is_alpha, token.is_stop)\n",
        "        print(\"-SUJETO-\")\n",
        "        print(\"Solucion 1 \", extrae_sujeto(doc))\n",
        "\n",
        "        #for ent in doc.ents:\n",
        "        #    print(ent.text, ent.start_char, ent.end_char, ent.label_)\n",
        "        print(\"-COMPLEMENTOS-\") # \n",
        "        #[to_nltk_tree(sent.root).pretty_print() for sent in doc.sents]\n",
        "        #sub_toks = [tok for tok in doc if (tok.dep_ == \"nsubj\") ]\n",
        "        #print(\"El sujeto de la oración es : \", sub_toks)\n",
        "        extrae_complementos(doc)\n",
        "        print(\"--------------\") # \n",
        "    if message.strip() == 'Bye':\n",
        "        print('ChatBot : Bye')\n",
        "        break\n",
        "for token in doc:\n",
        "    print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_,\n",
        "            token.shape_, token.is_alpha, token.is_stop)\n",
        "print(\"*************************************************\")    \n",
        "for ent in doc.ents:\n",
        "    print(ent.text, ent.start_char, ent.end_char, ent.label_)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " Version de Spacy es  2.2.4\n",
            "1mera prueva\n",
            "You: Cuantos años tienes\n",
            "********************\n",
            "-SUJETO-\n",
            "Solucion 1  []\n",
            "-COMPLEMENTOS-\n",
            "Cuantos años años ROOT años\n",
            "--------------\n",
            "You: Que tiempo hace\n",
            "********************\n",
            "-SUJETO-\n",
            "Candidato a sujeto tiempo\n",
            "Solucion 1  tiempo\n",
            "-COMPLEMENTOS-\n",
            "tiempo tiempo nsubj hace\n",
            "--------------\n",
            "You: Cuantos hijos tienes\n",
            "********************\n",
            "-SUJETO-\n",
            "Solucion 1  []\n",
            "-COMPLEMENTOS-\n",
            "Cuantos hijos hijos ROOT hijos\n",
            "--------------\n",
            "You: DE que color es el coche\n",
            "********************\n",
            "-SUJETO-\n",
            "Candidato a sujeto color\n",
            "Solucion 1  color\n",
            "-COMPLEMENTOS-\n",
            "color color nsubj coche\n",
            "el coche coche ROOT coche\n",
            "--------------\n",
            "You: Donde esta el coche\n",
            "********************\n",
            "-SUJETO-\n",
            "Solucion 1  []\n",
            "-COMPLEMENTOS-\n",
            "Donde Donde case coche\n",
            "el coche coche ROOT coche\n",
            "--------------\n",
            "You: Que hora es\n",
            "********************\n",
            "-SUJETO-\n",
            "Solucion 1  []\n",
            "-COMPLEMENTOS-\n",
            "hora hora ROOT hora\n",
            "--------------\n",
            "You: Esta el libro encima de la mesa\n",
            "********************\n",
            "-SUJETO-\n",
            "Solucion 1  []\n",
            "-COMPLEMENTOS-\n",
            "Esta Esta det el\n",
            "el libro libro ROOT libro\n",
            "la mesa mesa nmod libro\n",
            "--------------\n",
            "You: Como te llamas\n",
            "********************\n",
            "-SUJETO-\n",
            "Solucion 1  []\n",
            "-COMPLEMENTOS-\n",
            "te te obj llamas\n",
            "--------------\n",
            "You: De que color es la moto\n",
            "********************\n",
            "-SUJETO-\n",
            "Candidato a sujeto color\n",
            "Solucion 1  color\n",
            "-COMPLEMENTOS-\n",
            "color color nsubj moto\n",
            "la moto moto ROOT moto\n",
            "--------------\n",
            "You: Tienes la solucionç\n",
            "********************\n",
            "-SUJETO-\n",
            "Solucion 1  []\n",
            "-COMPLEMENTOS-\n",
            "la solucionç solucionç obj Tienes\n",
            "--------------\n",
            "You: Bye\n",
            "ChatBot : Bye\n",
            "Tienes Tienes VERB VERB__Mood=Ind|Number=Sing|Person=2|Tense=Pres|VerbForm=Fin ROOT Xxxxx True False\n",
            "la lo DET DET__Definite=Def|Gender=Fem|Number=Sing|PronType=Art det xx True True\n",
            "solucionç solucionç NOUN NOUN__Gender=Fem|Number=Sing obj xxxx True False\n",
            "*************************************************\n",
            "Tienes 0 6 PER\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fof-dXGgJaBE",
        "colab_type": "code",
        "outputId": "850b9773-2c26-4bb9-ac47-1f62c43add5f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "doc = nlp('el regreso desearimos llegar para date . la salida viajaria el date . el viajaria en tren . yo partiriamos el date . yo desplazarse a el_cabo . ')\n",
        "for sent in doc.sents:\n",
        "    #print(type(sent))\n",
        "    print(\"*\"*20)\n",
        "    my_story = sent.text\n",
        "    print(my_story)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "********************\n",
            "el regreso desearimos llegar para date .\n",
            "********************\n",
            "la salida viajaria el date .\n",
            "********************\n",
            "el viajaria en tren .\n",
            "********************\n",
            "yo partiriamos el date .\n",
            "********************\n",
            "yo desplazarse a el_cabo .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i-0wOvVD9FcE",
        "colab_type": "code",
        "outputId": "15e168c9-1074-4d88-b36b-40b41ba78dc9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        }
      },
      "source": [
        "target = nlp(\"Yo me llamo Fernando.\")\n",
        " \n",
        "doc1 = nlp(\"Los perror son increibles.\")\n",
        "doc2 = nlp(\"Ella es  Eduardo.\")\n",
        "doc3 = nlp(\"Los delfines son mamiferos.\")\n",
        " \n",
        "print(target.similarity(doc1))  \n",
        "print(target.similarity(doc2))  \n",
        "print(target.similarity(doc3))  "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.10212119821426445\n",
            "0.5313637848619017\n",
            "0.02375624216930202\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3yQU5n3owmLv",
        "colab_type": "code",
        "outputId": "8de32b09-9cdf-4a4a-b042-90ba0096584d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        }
      },
      "source": [
        "target = nlp(\"Los gatos son animales bonitos.\")\n",
        " \n",
        "doc1 = nlp(\"Los perror son increibles.\")\n",
        "doc2 = nlp(\"Algunos de los animales más bonitos son los felinos.\")\n",
        "doc3 = nlp(\"Los delfines son mamiferos.\")\n",
        " \n",
        "print(target.similarity(doc1))  # 0.8901765218466683\n",
        "print(target.similarity(doc2))  # 0.9115828449161616\n",
        "print(target.similarity(doc3))  # 0.7822956752876101"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.7987347901043897\n",
            "0.8327416287239938\n",
            "0.7846841765740403\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BT-bjBQ2Nv1s",
        "colab_type": "code",
        "outputId": "96a591c4-1198-4555-e875-b667d3078399",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "\n",
        "\n",
        "text1 = \"this is a sentence...hello...and another sentence.\"\n",
        "text = \"this is a sentence but I prefer this another sentence.\"\n",
        "text = \"Yo naci en Madrid and Ella murio en Barcelona\"\n",
        "\n",
        "\n",
        "doc = nlp(text)\n",
        "print(\"Before:\", [sent.text for sent in doc.sents])\n",
        "\n",
        "def set_custom_boundaries(doc):\n",
        "    for token in doc[:-1]:\n",
        "        if token.text == \"...\":\n",
        "            doc[token.i+1].is_sent_start = True\n",
        "    return doc\n",
        "\n",
        "#nlp.add_pipe(set_custom_boundaries, before=\"parser\")\n",
        "doc = nlp(text)\n",
        "print(\"After:\", [sent.text for sent in doc.sents])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Before: ['Yo naci en Madrid and Ella murio en Barcelona']\n",
            "After: ['Yo naci en Madrid and Ella murio en Barcelona']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bEueQL-RBx94",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Let's look at the named entities of this example:\n",
        "example = \"Apple's stocks dropped dramatically after the death of Steve Jobs in October.\"\n",
        "parsedEx = parser(example)\n",
        "for token in parsedEx:\n",
        "    print(token.orth_, token.ent_type_ if token.ent_type_ != \"\" else \"(not an entity)\")\n",
        "\n",
        "print(\"-------------- entities only ---------------\")\n",
        "# if you just want the entities and nothing else, you can do access the parsed examples \"ents\" property like this:\n",
        "ents = list(parsedEx.ents)\n",
        "for entity in ents:\n",
        "    print(entity.label, entity.label_, ' '.join(t.orth_ for t in entity))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nbNe9OJJDz_d",
        "colab_type": "code",
        "outputId": "38714cf9-1875-41c9-c4f7-0f9bb8874f6a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "# Finding a verb with a subject from below — good\n",
        "#def get_main_verbs_of_sent(sent):\n",
        "#    \"\"\"Return the main (non-auxiliary) verbs in a sentence.\"\"\"\n",
        "#    return [tok for tok in sent\n",
        "#            if tok.pos == VERB and tok.dep_ not in {'aux', 'auxpass'}]\n",
        "#import spacy\n",
        "#from spacy.symbols import nsubj, VERB\n",
        "#import es_core_news_md\n",
        "\n",
        "\n",
        "\n",
        "#nlp = es_core_news_md.load()\n",
        "\n",
        "\n",
        "doc = nlp(\"El cae hacia la mesa\")\n",
        "\n",
        "# Finding a verb with a subject from below — good\n",
        "verbs = set()\n",
        "for possible_subject in doc:\n",
        "    if possible_subject.dep == nsubj and possible_subject.head.pos == VERB:\n",
        "        verbs.add(possible_subject.head)\n",
        "    else:\n",
        "      print(\"nada\")\n",
        "print(verbs)\n",
        "print(\"*\"*20)\n",
        "print(\"Verbs:\", [token.lemma_ for token in doc if token.pos_ == \"VERB\"])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "nada\n",
            "nada\n",
            "nada\n",
            "nada\n",
            "nada\n",
            "set()\n",
            "********************\n",
            "Verbs: ['caer']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SJcX6NOMuwMs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp(\"Autonomous cars shift insurance liability toward manufacturers\")\n",
        "for chunk in doc.noun_chunks:\n",
        "    print(chunk.text, chunk.root.text, chunk.root.dep_,\n",
        "            chunk.root.head.text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P1AVilQzl4Pj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#######################################################################################################\n",
        "##\n",
        "##            NO FUNCIONA ----> PRUEBA CHUNCK\n",
        "##\n",
        "##\n",
        "#######################################################################################################\n",
        "import spacy\n",
        "\n",
        "def divide_frase(parsed_text):\n",
        "    #parsed_text = nlp(u\"I thought it was the complete set\")\n",
        "\n",
        "    #get token dependencies\n",
        "    for text in parsed_text:\n",
        "        #subject would be\n",
        "        if text.dep_ == \"nsubj\":\n",
        "            subject = text.orth_\n",
        "        #iobj for indirect object\n",
        "        if text.dep_ == \"iobj\":\n",
        "            indirect_object = text.orth_\n",
        "         #dobj for direct object\n",
        "        if text.dep_ == \"dobj\":\n",
        "            direct_object = text.orth_\n",
        "\n",
        "print(subject)\n",
        "print(direct_object)\n",
        "print(indirect_object)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5SyY4_joznRk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "text = \" En un lugar de la mancha\"\n",
        "\n",
        "vocab = sorted(set(text))\n",
        "print(vocab)\n",
        "print(\"*\"*20)\n",
        "vocab_to_int = {c: i for i, c in enumerate(vocab)}\n",
        "print(vocab_to_int)\n",
        "print(\"*\"*20)\n",
        "int_to_vocab = dict(enumerate(vocab))\n",
        "print(int_to_vocab)\n",
        "print(\"*\"*20)\n",
        "encoded = np.array([vocab_to_int[c] for c in text], dtype=np.int32)\n",
        "print(encoded)\n",
        "print(\"*\"*20)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZqqFHE_sbj83",
        "colab_type": "code",
        "outputId": "45a0eb8d-700c-473f-b96f-a417412981b2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 316
        }
      },
      "source": [
        "\n",
        "import es_core_news_lg"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-06f5b228f174>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mes_core_news_lg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'es_core_news_lg'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XYWhzZ7zZiYi",
        "colab_type": "code",
        "outputId": "f81a156f-bec4-48fc-9691-50de24f0c0db",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        }
      },
      "source": [
        "# BUSCANDO EL SUJETO DE LA FRASE CON SIMILARITY DE SPACE\n",
        "#\n",
        "\n",
        "\n",
        "#\n",
        "#\n",
        "import es_core_news_sm\n",
        "#nlp_ = spacy.load(\"en_core_web_sm\", vectors=False)\n",
        "nlp = es_core_news_sm.load()\n",
        "target = nlp(\"amo  arboles.\")\n",
        "target = nlp(\"caigo \")\n",
        "target = nlp(\"canto \")\n",
        " \n",
        "doc1 = nlp(\"yo monto  caballos.\")\n",
        "doc2 = nlp(\"tu montas  caballos.\")\n",
        "doc3 = nlp(\"ella monta  caballos.\")\n",
        "\n",
        "doc1 = nlp(\"yo parto\")\n",
        "doc2 = nlp(\"tu partes\")\n",
        "doc3 = nlp(\"el parte\")\n",
        "\n",
        "doc1 = nlp(\"yo amo\")\n",
        "doc2 = nlp(\"tu amas\")\n",
        "doc3 = nlp(\"el ama\")\n",
        "\n",
        "\n",
        "print(target.similarity(doc1))  \n",
        "print(target.similarity(doc2))  \n",
        "print(target.similarity(doc3))  \n",
        "#0.6842372198799445\n",
        "#0.6921089831474682\n",
        "#0.6194535262144631\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.497775422330826\n",
            "0.42346812427878167\n",
            "0.28280328226315526\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LZTb3d25i4Kx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "https://tfhub.dev/google/universal-sentence-encoder-multilingual-large/3\n",
        "!pip3 install tensorflow_text>=2.0.0rc0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xJr4LsGci2wC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow_hub as hub\n",
        "import numpy as np\n",
        "import tensorflow_text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-O_4yKInkZ8M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embed = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder-multilingual-large/3\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "13dIRyM1kj6Z",
        "colab_type": "code",
        "outputId": "a62fa438-75a0-4613-d537-b8cbce2d0d12",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "english_sentences = [\"Yo como\"]\n",
        "english_sentences2 = [\"El ama\"]\n",
        "english_sentences3 = [\"El come\"]\n",
        "\n",
        "#target = nlp(\"amo  arboles.\")\n",
        "my_sentences = [\"caigo \"]\n",
        "english_sentences = [\"yo parto\"]\n",
        "english_sentences2 = [\"tu partes \"]\n",
        "english_sentences3 = [\"el parte\"]\n",
        "\n",
        "to_analize = embed(my_sentences)\n",
        "result = embed(english_sentences)\n",
        "result2 = embed(english_sentences2)\n",
        "result3 = embed(english_sentences3)\n",
        "similarity_matrix_1 = np.inner(to_analize, result)\n",
        "similarity_matrix_2 = np.inner(to_analize, result2)\n",
        "similarity_matrix_3 = np.inner(to_analize, result3)\n",
        "print(similarity_matrix_1)\n",
        "print(similarity_matrix_2)\n",
        "print(similarity_matrix_3)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.35208154]]\n",
            "[[0.44740623]]\n",
            "[[0.40690267]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y8zCjEyGefbz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#https://github.com/chartbeat-labs/textacy\n",
        "from spacy.parts_of_speech import CONJ, DET, NOUN, VERB\n",
        "from spacy.tokens.span import Span as SpacySpan\n",
        "import itertools   \n",
        "#from . import compat\n",
        "#from . import constants\n",
        "#from . import spacy_utils\n",
        "#from . import text_utils\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zCtNELQxdW8M",
        "colab_type": "code",
        "outputId": "b6edf96a-95cb-4986-d9e9-fc7799056761",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        }
      },
      "source": [
        "def _get_conjuncts(tok):\n",
        "    \"\"\"\n",
        "    Return conjunct dependents of the leftmost conjunct in a coordinated phrase,\n",
        "    e.g. \"Burton, [Dan], and [Josh] ...\".\n",
        "    \"\"\"\n",
        "    return [right for right in tok.rights\n",
        "            if right.dep_ == 'conj']\n",
        "\n",
        "\n",
        "def get_main_verbs_of_sent(sent):\n",
        "    \"\"\"Return the main (non-auxiliary) verbs in a sentence.\"\"\"\n",
        "    return [tok for tok in sent\n",
        "            if tok.pos == VERB and tok.dep_ not in {'aux', 'auxpass'}]\n",
        "\n",
        "def get_subjects_of_verb(verb):\n",
        "    \"\"\"Return all subjects of a verb according to the dependency parse.\"\"\"\n",
        "    SUBJ_DEPS = {'agent', 'csubj', 'csubjpass', 'expl', 'nsubj', 'nsubjpass'}\n",
        "\n",
        "    subjs = [tok for tok in verb.lefts\n",
        "             if tok.dep_ in SUBJ_DEPS]\n",
        "    # get additional conjunct subjects\n",
        "    subjs.extend(tok for subj in subjs for tok in _get_conjuncts(subj))\n",
        "    return subjs\n",
        "\n",
        "def get_objects_of_verb(verb):\n",
        "    \"\"\"\n",
        "    Return all objects of a verb according to the dependency parse,\n",
        "    including open clausal complements.\n",
        "    \"\"\"\n",
        "    OBJ_DEPS = {'attr', 'dobj', 'dative', 'oprd'}\n",
        "    objs = [tok for tok in verb.rights\n",
        "            if tok.dep_ in OBJ_DEPS]\n",
        "    # get open clausal complements (xcomp)\n",
        "    objs.extend(tok for tok in verb.rights\n",
        "                if tok.dep_ == 'xcomp')\n",
        "    # get additional conjunct objects\n",
        "    objs.extend(tok for obj in objs for tok in _get_conjuncts(obj))\n",
        "    return objs\n",
        "\n",
        "def get_span_for_verb_auxiliaries(verb):\n",
        "    \"\"\"\n",
        "    Return document indexes spanning all (adjacent) tokens\n",
        "    around a verb that are auxiliary verbs or negations.\n",
        "    \"\"\"\n",
        "    AUX_DEPS = {'aux', 'auxpass', 'neg'}\n",
        "    min_i = verb.i - sum(1 for _ in itertools.takewhile(lambda x: x.dep_ in AUX_DEPS,\n",
        "                                                        reversed(list(verb.lefts))))\n",
        "    max_i = verb.i + sum(1 for _ in itertools.takewhile(lambda x: x.dep_ in AUX_DEPS,\n",
        "                                                        verb.rights))\n",
        "    return (min_i, max_i)\n",
        "\n",
        "\n",
        "def get_span_for_compound_noun(noun):\n",
        "    \"\"\"\n",
        "    Return document indexes spanning all (adjacent) tokens\n",
        "    in a compound noun.\n",
        "    \"\"\"\n",
        "    min_i = noun.i - sum(1 for _ in itertools.takewhile(lambda x: x.dep_ == 'compound',\n",
        "                                                        reversed(list(noun.lefts))))\n",
        "    return (min_i, noun.i)\n",
        "\n",
        "\n",
        "\n",
        "def subject_verb_object_triples(doc):\n",
        "  \n",
        "    if isinstance(doc, SpacySpan):\n",
        "        sents = [doc]\n",
        "    else:  # textacy.Doc or spacy.Doc\n",
        "        sents = doc.sents\n",
        "\n",
        "    for sent in sents:\n",
        "        start_i = sent[0].i\n",
        "\n",
        "        verbs = get_main_verbs_of_sent(sent)\n",
        "        for verb in verbs:\n",
        "            print(verb)\n",
        "            subjs = get_subjects_of_verb(verb)\n",
        "            print(subjs)\n",
        "            #if not subjs:\n",
        "            #    continue\n",
        "            objs = get_objects_of_verb(verb)\n",
        "            print(objs)\n",
        "            #if not objs:\n",
        "            #    continue\n",
        "            # add adjacent auxiliaries to verbs, for context\n",
        "            # and add compounds to compound nouns\n",
        "            \n",
        "            verb_span = get_span_for_verb_auxiliaries(verb)\n",
        "            verb = sent[verb_span[0] - start_i: verb_span[1] - start_i + 1]\n",
        "            for subj in subjs:\n",
        "                subj = sent[get_span_for_compound_noun(subj)[0] - start_i: subj.i - start_i + 1]\n",
        "                print(subj)\n",
        "                for obj in objs:\n",
        "                    if obj.pos == NOUN:\n",
        "                        span = get_span_for_compound_noun(obj)\n",
        "                    elif obj.pos == VERB:\n",
        "                        span = get_span_for_verb_auxiliaries(obj)\n",
        "                    else:\n",
        "                        span = (obj.i, obj.i)\n",
        "                    obj = sent[span[0] - start_i: span[1] - start_i + 1]\n",
        "\n",
        "    return (subj, verb, obj)\n",
        "\n",
        "import es_core_news_sm\n",
        "nlp = es_core_news_sm.load()\n",
        "\n",
        "\n",
        "doc = nlp(\"El libro estudia matematicas en la mesa contigo\")\n",
        "print(subject_verb_object_triples(doc))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "estudia\n",
            "[libro]\n",
            "[]\n",
            "libro\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "UnboundLocalError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-44-29fd7f87f672>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"El libro estudia matematicas en la mesa contigo\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubject_verb_object_triples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-44-29fd7f87f672>\u001b[0m in \u001b[0;36msubject_verb_object_triples\u001b[0;34m(doc)\u001b[0m\n\u001b[1;32m    100\u001b[0m                     \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msent\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mspan\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_i\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mspan\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_i\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msubj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mes_core_news_sm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'obj' referenced before assignment"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y6wWYNcrG4KK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#https://github.com/Dimev/Spacy-SVO-extraction/blob/master/main.py\n",
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# object and subject constants\n",
        "OBJECT_DEPS = {\"dobj\", \"dative\", \"attr\", \"oprd\"}\n",
        "SUBJECT_DEPS = {\"nsubj\", \"nsubjpass\", \"csubj\", \"agent\", \"expl\"}\n",
        "# tags that define wether the word is wh-\n",
        "WH_WORDS = {\"WP\", \"WP$\", \"WRB\"}\n",
        "\n",
        "# extract the subject, object and verb from the input\n",
        "def extract_svo(doc):\n",
        "    sub = []\n",
        "    at = []\n",
        "    ve = []\n",
        "    for token in doc:\n",
        "        # is this a verb?\n",
        "        if token.pos_ == \"VERB\":\n",
        "            ve.append(token.text)\n",
        "        # is this the object?\n",
        "        if token.dep_ in OBJECT_DEPS or token.head.dep_ in OBJECT_DEPS:\n",
        "            at.append(token.text)\n",
        "        # is this the subject?\n",
        "        if token.dep_ in SUBJECT_DEPS or token.head.dep_ in SUBJECT_DEPS:\n",
        "            sub.append(token.text)\n",
        "    return \" \".join(sub).strip().lower(), \" \".join(ve).strip().lower(), \" \".join(at).strip().lower()\n",
        "\n",
        "# wether the doc is a question, as well as the wh-word if any\n",
        "def is_question(doc):\n",
        "    # is the first token a verb?\n",
        "    if len(doc) > 0 and doc[0].pos_ == \"VERB\":\n",
        "        return True, \"\"\n",
        "    # go over all words\n",
        "    for token in doc:\n",
        "        # is it a wh- word?\n",
        "        if token.tag_ in WH_WORDS:\n",
        "            return True, token.text.lower()\n",
        "    return False, \"\"\n",
        "\n",
        "# gather the user input and gather the info\n",
        "while True:    \n",
        "    doc = nlp(input(\"> \"))\n",
        "    # print out the pos and deps\n",
        "    for token in doc:\n",
        "        print(\"Token {} POS: {}, dep: {}\".format(token.text, token.pos_, token.dep_))\n",
        "\n",
        "    # get the input information\n",
        "    subject, verb, attribute = extract_svo(doc)\n",
        "    question, wh_word = is_question(doc)\n",
        "    print(\"svo:, subject: {}, verb: {}, attribute: {}, question: {}, wh_word: {}\".format(subject, verb, attribute, question, wh_word))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}